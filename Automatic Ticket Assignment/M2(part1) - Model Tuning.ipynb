{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2(part1) - Model Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuUSC7LALrbD"
      },
      "source": [
        "### **Starting with Milestone 2**\r\n",
        "\r\n",
        "**In Milestone 1 last part (part2b) we concluded that we will take forward lstm model with pre-trained GloVe embedding and improve upon it for this milestone. We created the most basic LSTM model. The architecture of that model is as follows:**<br>\r\n",
        "- input layer of size 76 (x_train.shape[1])<br>\r\n",
        "- embedding layer with input_dim = size of the vocabulary, output_dim = 200, weight=GloveEmbeddingMatrix, input length = size of input i.e x_train.shape[1] and trainable=True<br>\r\n",
        "- LSTM layer with 100 units and return sequence = False<br>\r\n",
        "- Output dense layer with units = # labels in the target i.e 74 (y_train.shape[1])<br><br>\r\n",
        "\r\n",
        "\r\n",
        "In this Milestone, We will improve the architecture as well as tune the hyper-parameters of the architecture. **This is how we plan it:**<br>\r\n",
        "**First we will improve upon the architecture:**<br>\r\n",
        "- turn on return sequences<br>\r\n",
        "- add some regularization such as dropouts<br>\r\n",
        "- introduce bi-directional layer<br>\r\n",
        "- add more LSTM units<br><br>\r\n",
        "\r\n",
        "**After deciding the architecture we will tune for better hyper-parameters:**<br>\r\n",
        "- try a different optimizer<br>\r\n",
        "- try to hit the right learning rate<br>\r\n",
        "- implement callbacks in order to train for more epochs and account for overfitting\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVORFjbwTgTa",
        "outputId": "b19ce73e-b594-41ec-84f1-7e1a9a603c49"
      },
      "source": [
        "import sys\r\n",
        "sys.path.append('/content/drive/MyDrive/Automatic Ticket Assignment')\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETFXRGLsS_Mx"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input, GlobalMaxPool1D, Flatten\r\n",
        "from Model.DLModelTuningAndEvaluation import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6ptmGL2TAOp"
      },
      "source": [
        "**First we will load GloVeEmbeddingMatrix, train set, train labels, test set and test labels that we saved in Milestone 1 part2b:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgttYkIbtm4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9977efca-dd23-4260-ddde-40a87973224a"
      },
      "source": [
        "# USE THIS SAVEPATH IF RUNNING IN GOOGLE COLAB, GIVE THE PATH WHERE YOU WANT TO SAVE\r\n",
        "SAVEPATH = '/content/drive/MyDrive/Automatic Ticket Assignment/DataFiles/Milestone2/'\r\n",
        "# *************************** --------------------------************************************\r\n",
        "# SAVEPATH = 'DataFiles/Milestone2/'\r\n",
        "\r\n",
        "embedding_matrix = np.load(SAVEPATH+'GloveEmbeddingMatrix.npy')\r\n",
        "x_train = np.load(SAVEPATH+'xtrain.npy')\r\n",
        "y_train = np.load(SAVEPATH+'ytrain.npy')\r\n",
        "x_test = np.load(SAVEPATH+'xtest.npy')\r\n",
        "y_test = np.load(SAVEPATH+'ytest.npy')\r\n",
        "\r\n",
        "print('JUST TO RECALL\\n','train set:',x_train.shape)\r\n",
        "print('train labels:',y_train.shape)\r\n",
        "print('test set:',x_test.shape)\r\n",
        "print('test labels:',y_test.shape,'\\n')\r\n",
        "print('embedding matrix:',embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JUST TO RECALL\n",
            " train set: (6432, 76)\n",
            "train labels: (6432, 74)\n",
            "test set: (1608, 76)\n",
            "test labels: (1608, 74) \n",
            "\n",
            "embedding matrix: (19235, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LsTgElGVkHi"
      },
      "source": [
        "**MODEL 2:**<BR>\r\n",
        "**So we will start improving upon the model architecture using the above plan:**<br>\r\n",
        "**Turn on return sequences**<br>\r\n",
        "When we configure return_sequences = True, the model returns the lstm's hidden state at every timestamp (at every word of the sequence). We have 76 words in every sequence so the Lstm layer output shape would be (None,76,100) as opposed to (None,100) when return_sequences = False. To maintain the shape integrity for the output layer we will need to flatten the input coming from lstm layer before passing to the output layer. **This can be done in two ways:** Either we can use the **dense layer** or a pool layer to flatten it; whichever gives the best result.<br>\r\n",
        "**Theoretically with return_sequences = true, the output of the hidden state is used as an input to the next LSTM layer at every timestamp**<br><br>\r\n",
        "**Continuing on model1b from last part, we will train each model for 15 epochs to gauge any improvements. First we will flatten with dense layer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiO46lOWTtMb",
        "outputId": "d358942c-c7b0-4197-b25c-ad403a69154f"
      },
      "source": [
        "# model2 architecture\r\n",
        "input = Input(shape=(x_train.shape[1],),batch_size=None)\r\n",
        "model2 = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train.shape[1], trainable=True)(input)\r\n",
        "model2 = LSTM(units=100, return_sequences=True)(model2) # return_sequences=True\r\n",
        "model2 = Flatten()(model2) # flattening to maintain shape integrity via dense layer\r\n",
        "out = Dense(y_train.shape[1], activation=\"softmax\")(model2)\r\n",
        "model2 = Model(input, out)\r\n",
        "model2.summary()\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "# train model2\r\n",
        "train_model(model2, x_train, y_train, x_test, y_test, ep=15, bs=16)#66.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 76)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 76, 200)           3847000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 76, 100)           120400    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 7600)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 74)                562474    \n",
            "=================================================================\n",
            "Total params: 4,529,874\n",
            "Trainable params: 4,529,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "Epoch 1/15\n",
            "402/402 [==============================] - 36s 86ms/step - loss: 2.4791 - accuracy: 0.4793 - val_loss: 1.5692 - val_accuracy: 0.5964\n",
            "Epoch 2/15\n",
            "402/402 [==============================] - 34s 84ms/step - loss: 1.0852 - accuracy: 0.7109 - val_loss: 1.4578 - val_accuracy: 0.6486\n",
            "Epoch 3/15\n",
            "402/402 [==============================] - 34s 84ms/step - loss: 0.5987 - accuracy: 0.8387 - val_loss: 1.5363 - val_accuracy: 0.6536\n",
            "Epoch 4/15\n",
            "402/402 [==============================] - 34s 84ms/step - loss: 0.3844 - accuracy: 0.9088 - val_loss: 1.6383 - val_accuracy: 0.6692\n",
            "Epoch 5/15\n",
            "402/402 [==============================] - 35s 87ms/step - loss: 0.2626 - accuracy: 0.9427 - val_loss: 1.7261 - val_accuracy: 0.6673\n",
            "Epoch 6/15\n",
            "402/402 [==============================] - 34s 85ms/step - loss: 0.2042 - accuracy: 0.9552 - val_loss: 1.8860 - val_accuracy: 0.6374\n",
            "Epoch 7/15\n",
            "402/402 [==============================] - 39s 96ms/step - loss: 0.1421 - accuracy: 0.9693 - val_loss: 1.9558 - val_accuracy: 0.6617\n",
            "Epoch 8/15\n",
            "402/402 [==============================] - 34s 85ms/step - loss: 0.1159 - accuracy: 0.9741 - val_loss: 2.0095 - val_accuracy: 0.6343\n",
            "Epoch 9/15\n",
            "402/402 [==============================] - 34s 85ms/step - loss: 0.0979 - accuracy: 0.9771 - val_loss: 2.1294 - val_accuracy: 0.6486\n",
            "Epoch 10/15\n",
            "402/402 [==============================] - 35s 86ms/step - loss: 0.0992 - accuracy: 0.9757 - val_loss: 1.8766 - val_accuracy: 0.6449\n",
            "Epoch 11/15\n",
            "402/402 [==============================] - 34s 85ms/step - loss: 0.0790 - accuracy: 0.9788 - val_loss: 2.1297 - val_accuracy: 0.6517\n",
            "Epoch 12/15\n",
            "402/402 [==============================] - 34s 86ms/step - loss: 0.0767 - accuracy: 0.9801 - val_loss: 2.1581 - val_accuracy: 0.6499\n",
            "Epoch 13/15\n",
            "402/402 [==============================] - 34s 85ms/step - loss: 0.0672 - accuracy: 0.9798 - val_loss: 2.2119 - val_accuracy: 0.6493\n",
            "Epoch 14/15\n",
            "402/402 [==============================] - 35s 87ms/step - loss: 0.0777 - accuracy: 0.9778 - val_loss: 2.0491 - val_accuracy: 0.6474\n",
            "Epoch 15/15\n",
            "402/402 [==============================] - 34s 85ms/step - loss: 0.0793 - accuracy: 0.9760 - val_loss: 2.0516 - val_accuracy: 0.6169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7f703f656c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qc9fhJmxyqh"
      },
      "source": [
        "**MODEL 3:**<br>Using **GlobalMaxPooling layer** to flatten the input coming from LSTM layer that will go in the output layer instead of dense layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7K6AIjzUlco",
        "outputId": "6e87ad55-191d-4878-c4fd-75498c7776fe"
      },
      "source": [
        "# model3 architecture\r\n",
        "input = Input(shape=(x_train.shape[1],),batch_size=None)\r\n",
        "model3 = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train.shape[1], trainable=True)(input)\r\n",
        "model3 = LSTM(units=100, return_sequences=True)(model3) # return_sequences=true \r\n",
        "model3 = GlobalMaxPool1D()(model3) # flattening to maintain shape integrity via global max pooling\r\n",
        "out = Dense(y_train.shape[1], activation=\"softmax\")(model3)\r\n",
        "model3 = Model(input, out)\r\n",
        "model3.summary()\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "# train model3\r\n",
        "train_model(model3, x_train, y_train, x_test, y_test, ep=15, bs=16)#68.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 76)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 76, 200)           3847000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 76, 100)           120400    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 74)                7474      \n",
            "=================================================================\n",
            "Total params: 3,974,874\n",
            "Trainable params: 3,974,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "Epoch 1/15\n",
            "402/402 [==============================] - 36s 85ms/step - loss: 2.7355 - accuracy: 0.4392 - val_loss: 1.9821 - val_accuracy: 0.5404\n",
            "Epoch 2/15\n",
            "402/402 [==============================] - 33s 83ms/step - loss: 1.7623 - accuracy: 0.5828 - val_loss: 1.5816 - val_accuracy: 0.6119\n",
            "Epoch 3/15\n",
            "402/402 [==============================] - 33s 83ms/step - loss: 1.2970 - accuracy: 0.6797 - val_loss: 1.4308 - val_accuracy: 0.6362\n",
            "Epoch 4/15\n",
            "402/402 [==============================] - 33s 83ms/step - loss: 0.9222 - accuracy: 0.7780 - val_loss: 1.3310 - val_accuracy: 0.6648\n",
            "Epoch 5/15\n",
            "402/402 [==============================] - 33s 82ms/step - loss: 0.6267 - accuracy: 0.8431 - val_loss: 1.3778 - val_accuracy: 0.6741\n",
            "Epoch 6/15\n",
            "402/402 [==============================] - 33s 82ms/step - loss: 0.4491 - accuracy: 0.8967 - val_loss: 1.4032 - val_accuracy: 0.6667\n",
            "Epoch 7/15\n",
            "402/402 [==============================] - 33s 82ms/step - loss: 0.3190 - accuracy: 0.9279 - val_loss: 1.3993 - val_accuracy: 0.6754\n",
            "Epoch 8/15\n",
            "402/402 [==============================] - 33s 83ms/step - loss: 0.2195 - accuracy: 0.9518 - val_loss: 1.4727 - val_accuracy: 0.6623\n",
            "Epoch 9/15\n",
            "402/402 [==============================] - 33s 82ms/step - loss: 0.1931 - accuracy: 0.9567 - val_loss: 1.4794 - val_accuracy: 0.6716\n",
            "Epoch 10/15\n",
            "402/402 [==============================] - 33s 82ms/step - loss: 0.1473 - accuracy: 0.9662 - val_loss: 1.4905 - val_accuracy: 0.6698\n",
            "Epoch 11/15\n",
            "402/402 [==============================] - 33s 83ms/step - loss: 0.1049 - accuracy: 0.9760 - val_loss: 1.5583 - val_accuracy: 0.6642\n",
            "Epoch 12/15\n",
            "402/402 [==============================] - 33s 82ms/step - loss: 0.0862 - accuracy: 0.9793 - val_loss: 1.6152 - val_accuracy: 0.6617\n",
            "Epoch 13/15\n",
            "402/402 [==============================] - 33s 82ms/step - loss: 0.0874 - accuracy: 0.9790 - val_loss: 1.6411 - val_accuracy: 0.6586\n",
            "Epoch 14/15\n",
            "402/402 [==============================] - 33s 82ms/step - loss: 0.0735 - accuracy: 0.9793 - val_loss: 1.6569 - val_accuracy: 0.6517\n",
            "Epoch 15/15\n",
            "402/402 [==============================] - 33s 83ms/step - loss: 0.0764 - accuracy: 0.9767 - val_loss: 1.6779 - val_accuracy: 0.6623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7f70393f5a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWd-ZR7CW8hO"
      },
      "source": [
        "**Clearly flattening with pooling gives better validation accuracy and loss. Also since its not a dense layer, it has much less parameters to train which means faster training. Hence we will add pooling layer to the architecture**<br><br>\r\n",
        "**MODEL 4:**<br>\r\n",
        "Next up is **Regularisation:**<br>\r\n",
        "Adding recurrent dropout to the lstm layer and a dropout layer after pooling with 10% dropout rate for both"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a9KAVNiUlhi",
        "outputId": "ee3ee78e-e10c-447b-f0c5-b365a98306cc"
      },
      "source": [
        "# model4 architecture\r\n",
        "input = Input(shape=(x_train.shape[1],),batch_size=None)\r\n",
        "model4 = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train.shape[1], trainable=True)(input)\r\n",
        "model4 = LSTM(units=100, return_sequences=True,recurrent_dropout=0.1)(model4) #recurrent dropout\r\n",
        "model4 = GlobalMaxPool1D()(model4)\r\n",
        "model4 = Dropout(0.1)(model4) #dropout layer\r\n",
        "out = Dense(y_train.shape[1], activation=\"softmax\")(model4)\r\n",
        "model4 = Model(input, out)\r\n",
        "model4.summary()\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "# train model4\r\n",
        "train_model(model4, x_train, y_train, x_test, y_test, ep=15, bs=16)#69.09"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 76)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 76, 200)           3847000   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 76, 100)           120400    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 74)                7474      \n",
            "=================================================================\n",
            "Total params: 3,974,874\n",
            "Trainable params: 3,974,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "Epoch 1/15\n",
            "402/402 [==============================] - 54s 131ms/step - loss: 2.7770 - accuracy: 0.4465 - val_loss: 1.9200 - val_accuracy: 0.5591\n",
            "Epoch 2/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 1.7249 - accuracy: 0.5961 - val_loss: 1.5755 - val_accuracy: 0.6169\n",
            "Epoch 3/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 1.2958 - accuracy: 0.6799 - val_loss: 1.3989 - val_accuracy: 0.6511\n",
            "Epoch 4/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.9943 - accuracy: 0.7533 - val_loss: 1.3574 - val_accuracy: 0.6623\n",
            "Epoch 5/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 0.6933 - accuracy: 0.8343 - val_loss: 1.3522 - val_accuracy: 0.6586\n",
            "Epoch 6/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.5056 - accuracy: 0.8773 - val_loss: 1.3497 - val_accuracy: 0.6660\n",
            "Epoch 7/15\n",
            "402/402 [==============================] - 53s 133ms/step - loss: 0.3551 - accuracy: 0.9197 - val_loss: 1.3486 - val_accuracy: 0.6710\n",
            "Epoch 8/15\n",
            "402/402 [==============================] - 54s 134ms/step - loss: 0.2649 - accuracy: 0.9440 - val_loss: 1.4517 - val_accuracy: 0.6735\n",
            "Epoch 9/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 0.2019 - accuracy: 0.9521 - val_loss: 1.4503 - val_accuracy: 0.6648\n",
            "Epoch 10/15\n",
            "402/402 [==============================] - 53s 133ms/step - loss: 0.1461 - accuracy: 0.9680 - val_loss: 1.5475 - val_accuracy: 0.6573\n",
            "Epoch 11/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.1387 - accuracy: 0.9673 - val_loss: 1.6096 - val_accuracy: 0.6729\n",
            "Epoch 12/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 0.1057 - accuracy: 0.9729 - val_loss: 1.5825 - val_accuracy: 0.6617\n",
            "Epoch 13/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 0.0914 - accuracy: 0.9790 - val_loss: 1.7227 - val_accuracy: 0.6474\n",
            "Epoch 14/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 0.0839 - accuracy: 0.9791 - val_loss: 1.6085 - val_accuracy: 0.6673\n",
            "Epoch 15/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.0866 - accuracy: 0.9754 - val_loss: 1.6759 - val_accuracy: 0.6710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7f703881b390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb6lhu300n6D"
      },
      "source": [
        "**MODEL 5:**<br>\r\n",
        "Increasing the dropout rate to **20%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dxnAiUQLQIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3a3554-89ac-42e4-bef8-6643d26b948e"
      },
      "source": [
        "# model5 architecture \r\n",
        "input = Input(shape=(x_train.shape[1],),batch_size=None)\r\n",
        "model5 = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train.shape[1], trainable=True)(input)\r\n",
        "model5 = LSTM(units=100, return_sequences=True,recurrent_dropout=0.2)(model5) #recurrent dropout\r\n",
        "model5 = GlobalMaxPool1D()(model5)\r\n",
        "model5 = Dropout(0.2)(model5) #dropout layer\r\n",
        "out = Dense(y_train.shape[1], activation=\"softmax\")(model5)\r\n",
        "model5 = Model(input, out)\r\n",
        "model5.summary()\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "# train model5\r\n",
        "train_model(model5, x_train, y_train, x_test, y_test, ep=15, bs=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 76)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 76, 200)           3847000   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 76, 100)           120400    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 74)                7474      \n",
            "=================================================================\n",
            "Total params: 3,974,874\n",
            "Trainable params: 3,974,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "Epoch 1/15\n",
            "402/402 [==============================] - 55s 133ms/step - loss: 2.7891 - accuracy: 0.4364 - val_loss: 1.9853 - val_accuracy: 0.5485\n",
            "Epoch 2/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 1.8129 - accuracy: 0.5844 - val_loss: 1.6038 - val_accuracy: 0.6138\n",
            "Epoch 3/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 1.3813 - accuracy: 0.6642 - val_loss: 1.4274 - val_accuracy: 0.6493\n",
            "Epoch 4/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 1.0389 - accuracy: 0.7398 - val_loss: 1.3919 - val_accuracy: 0.6754\n",
            "Epoch 5/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 0.7861 - accuracy: 0.8071 - val_loss: 1.3311 - val_accuracy: 0.6816\n",
            "Epoch 6/15\n",
            "402/402 [==============================] - 53s 132ms/step - loss: 0.5969 - accuracy: 0.8544 - val_loss: 1.3344 - val_accuracy: 0.6772\n",
            "Epoch 7/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.4523 - accuracy: 0.8946 - val_loss: 1.3571 - val_accuracy: 0.6810\n",
            "Epoch 8/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.3442 - accuracy: 0.9218 - val_loss: 1.3712 - val_accuracy: 0.6816\n",
            "Epoch 9/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.2686 - accuracy: 0.9422 - val_loss: 1.4301 - val_accuracy: 0.6710\n",
            "Epoch 10/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.2332 - accuracy: 0.9455 - val_loss: 1.4385 - val_accuracy: 0.6716\n",
            "Epoch 11/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.1820 - accuracy: 0.9592 - val_loss: 1.4971 - val_accuracy: 0.6766\n",
            "Epoch 12/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.1557 - accuracy: 0.9632 - val_loss: 1.5516 - val_accuracy: 0.6822\n",
            "Epoch 13/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.1284 - accuracy: 0.9663 - val_loss: 1.5686 - val_accuracy: 0.6803\n",
            "Epoch 14/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.1194 - accuracy: 0.9694 - val_loss: 1.6708 - val_accuracy: 0.6741\n",
            "Epoch 15/15\n",
            "402/402 [==============================] - 53s 131ms/step - loss: 0.1100 - accuracy: 0.9742 - val_loss: 1.6296 - val_accuracy: 0.6810\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7f70372c0eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQQz8yjzfBiD"
      },
      "source": [
        "**Slightly better performance with dropout rate of 20% Hence we will go ahead with model 5 and add onto it in the upcoming models**<br><br>\r\n",
        "**MODEL 6:**<br>\r\n",
        "**Adding Bi-directional Layer:**<br>\r\n",
        "Bi-directional duplicates the first lstm layer in the network and puts the two layers side-by-side. The input sequence is fed as it is to the first layer while reversed copy of the input is fed to the second layer.<br>In simpler words the **bi-directional layer computes the inputs in two ways: past to future and future to past.** The idea behind bi-directional layer is to have better context while computing the word at any given timestamp. Due to computation of sequence and reverse sequence, at any given timestamp, lstm will have the context not only from the previous words but also from the words that are yet to come in future in that sequence.<br>**Now since we will add a bi-directional layer on lstm with 100 units, the total units will be 200 in our case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onEANekRG75b",
        "outputId": "f29c76d2-e826-47a5-be81-425d6190ab46"
      },
      "source": [
        "# model6 architecture\r\n",
        "input = Input(shape=(x_train.shape[1],),batch_size=None)\r\n",
        "model6 = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train.shape[1], trainable=True)(input)\r\n",
        "model6 = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2))(model6) #adding bi-directional layer\r\n",
        "model6 = GlobalMaxPool1D()(model6)\r\n",
        "model6 = Dropout(0.2)(model6)\r\n",
        "out = Dense(y_train.shape[1], activation=\"softmax\")(model6)\r\n",
        "model6 = Model(input, out)\r\n",
        "model6.summary()\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "# train model6\r\n",
        "train_model(model6, x_train, y_train, x_test, y_test, ep=15, bs=16)#68.59"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 76)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 76, 200)           3847000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 76, 200)           240800    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 74)                14874     \n",
            "=================================================================\n",
            "Total params: 4,102,674\n",
            "Trainable params: 4,102,674\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "Epoch 1/15\n",
            "402/402 [==============================] - 121s 295ms/step - loss: 2.6452 - accuracy: 0.4542 - val_loss: 1.7791 - val_accuracy: 0.5933\n",
            "Epoch 2/15\n",
            "402/402 [==============================] - 116s 290ms/step - loss: 1.6084 - accuracy: 0.6179 - val_loss: 1.4527 - val_accuracy: 0.6405\n",
            "Epoch 3/15\n",
            "402/402 [==============================] - 115s 287ms/step - loss: 1.1124 - accuracy: 0.7223 - val_loss: 1.2975 - val_accuracy: 0.6760\n",
            "Epoch 4/15\n",
            "402/402 [==============================] - 115s 286ms/step - loss: 0.8173 - accuracy: 0.7874 - val_loss: 1.2680 - val_accuracy: 0.6729\n",
            "Epoch 5/15\n",
            "402/402 [==============================] - 116s 289ms/step - loss: 0.6114 - accuracy: 0.8503 - val_loss: 1.2972 - val_accuracy: 0.6841\n",
            "Epoch 6/15\n",
            "402/402 [==============================] - 117s 291ms/step - loss: 0.3837 - accuracy: 0.9109 - val_loss: 1.2906 - val_accuracy: 0.6791\n",
            "Epoch 7/15\n",
            "402/402 [==============================] - 118s 295ms/step - loss: 0.2826 - accuracy: 0.9353 - val_loss: 1.3791 - val_accuracy: 0.6828\n",
            "Epoch 8/15\n",
            "402/402 [==============================] - 121s 300ms/step - loss: 0.2376 - accuracy: 0.9462 - val_loss: 1.4064 - val_accuracy: 0.6859\n",
            "Epoch 9/15\n",
            "402/402 [==============================] - 119s 295ms/step - loss: 0.1502 - accuracy: 0.9677 - val_loss: 1.4390 - val_accuracy: 0.6816\n",
            "Epoch 10/15\n",
            "402/402 [==============================] - 119s 295ms/step - loss: 0.1259 - accuracy: 0.9675 - val_loss: 1.5120 - val_accuracy: 0.6760\n",
            "Epoch 11/15\n",
            "402/402 [==============================] - 119s 296ms/step - loss: 0.1041 - accuracy: 0.9755 - val_loss: 1.5589 - val_accuracy: 0.6673\n",
            "Epoch 12/15\n",
            "402/402 [==============================] - 119s 296ms/step - loss: 0.1108 - accuracy: 0.9707 - val_loss: 1.5901 - val_accuracy: 0.6816\n",
            "Epoch 13/15\n",
            "402/402 [==============================] - 119s 296ms/step - loss: 0.0904 - accuracy: 0.9777 - val_loss: 1.6834 - val_accuracy: 0.6816\n",
            "Epoch 14/15\n",
            "402/402 [==============================] - 119s 297ms/step - loss: 0.0874 - accuracy: 0.9738 - val_loss: 1.6059 - val_accuracy: 0.6810\n",
            "Epoch 15/15\n",
            "402/402 [==============================] - 119s 295ms/step - loss: 0.0823 - accuracy: 0.9738 - val_loss: 1.6924 - val_accuracy: 0.6648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7fa664150278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXki0wMd1xnj"
      },
      "source": [
        "**More or less same performance as with model 5... but it takes DOUBLE the amount of time to train which is self explanatory; we have double the LSTM units. Since there is no difference in performance, therefore we retain model 5**<br><br>\r\n",
        "**Model 7:**<br>\r\n",
        "We rejected model 6 and retained model 5, hence in model 7 we will retain the architecture of model 5 **except add more LSTM units**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z06LU1ZxzD2L",
        "outputId": "a211cafb-963d-4ff4-d1f3-b5dd3f8a28ab"
      },
      "source": [
        "# model7 architecture\r\n",
        "input = Input(shape=(x_train.shape[1],),batch_size=None)\r\n",
        "model7 = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train.shape[1], trainable=True)(input)\r\n",
        "model7 = LSTM(units=150, return_sequences=True, recurrent_dropout=0.2)(model7)\r\n",
        "model7 = GlobalMaxPool1D()(model7)\r\n",
        "model7 = Dropout(0.2)(model7)\r\n",
        "out = Dense(y_train.shape[1], activation=\"softmax\")(model7)\r\n",
        "model7 = Model(input, out)\r\n",
        "model7.summary()\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "# train model7\r\n",
        "train_model(model7, x_train, y_train, x_test, y_test, ep=16, bs=16)#68.16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 76)]              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 76, 200)           3847000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 76, 150)           210600    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 74)                11174     \n",
            "=================================================================\n",
            "Total params: 4,068,774\n",
            "Trainable params: 4,068,774\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "Epoch 1/16\n",
            "402/402 [==============================] - 109s 267ms/step - loss: 2.7095 - accuracy: 0.4503 - val_loss: 1.8484 - val_accuracy: 0.5765\n",
            "Epoch 2/16\n",
            "402/402 [==============================] - 106s 265ms/step - loss: 1.7130 - accuracy: 0.5866 - val_loss: 1.5270 - val_accuracy: 0.6200\n",
            "Epoch 3/16\n",
            "402/402 [==============================] - 107s 266ms/step - loss: 1.2681 - accuracy: 0.6771 - val_loss: 1.3920 - val_accuracy: 0.6455\n",
            "Epoch 4/16\n",
            "402/402 [==============================] - 113s 282ms/step - loss: 0.9686 - accuracy: 0.7576 - val_loss: 1.2919 - val_accuracy: 0.6772\n",
            "Epoch 5/16\n",
            "402/402 [==============================] - 107s 266ms/step - loss: 0.6617 - accuracy: 0.8336 - val_loss: 1.2952 - val_accuracy: 0.6692\n",
            "Epoch 6/16\n",
            "402/402 [==============================] - 107s 267ms/step - loss: 0.5054 - accuracy: 0.8729 - val_loss: 1.3261 - val_accuracy: 0.6748\n",
            "Epoch 7/16\n",
            "402/402 [==============================] - 107s 265ms/step - loss: 0.3685 - accuracy: 0.9132 - val_loss: 1.3398 - val_accuracy: 0.6891\n",
            "Epoch 8/16\n",
            "402/402 [==============================] - 107s 266ms/step - loss: 0.2688 - accuracy: 0.9351 - val_loss: 1.3920 - val_accuracy: 0.6828\n",
            "Epoch 9/16\n",
            "402/402 [==============================] - 106s 265ms/step - loss: 0.2178 - accuracy: 0.9498 - val_loss: 1.4607 - val_accuracy: 0.6791\n",
            "Epoch 10/16\n",
            "402/402 [==============================] - 107s 266ms/step - loss: 0.1591 - accuracy: 0.9612 - val_loss: 1.5087 - val_accuracy: 0.6810\n",
            "Epoch 11/16\n",
            "402/402 [==============================] - 107s 267ms/step - loss: 0.1360 - accuracy: 0.9677 - val_loss: 1.5357 - val_accuracy: 0.6822\n",
            "Epoch 12/16\n",
            "402/402 [==============================] - 108s 267ms/step - loss: 0.1237 - accuracy: 0.9644 - val_loss: 1.5891 - val_accuracy: 0.6853\n",
            "Epoch 13/16\n",
            "402/402 [==============================] - 107s 266ms/step - loss: 0.1096 - accuracy: 0.9713 - val_loss: 1.5974 - val_accuracy: 0.6878\n",
            "Epoch 14/16\n",
            "402/402 [==============================] - 107s 265ms/step - loss: 0.0977 - accuracy: 0.9733 - val_loss: 1.6035 - val_accuracy: 0.6797\n",
            "Epoch 15/16\n",
            "402/402 [==============================] - 107s 267ms/step - loss: 0.0931 - accuracy: 0.9741 - val_loss: 1.6527 - val_accuracy: 0.6835\n",
            "Epoch 16/16\n",
            "402/402 [==============================] - 107s 265ms/step - loss: 0.0798 - accuracy: 0.9774 - val_loss: 1.7135 - val_accuracy: 0.6891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7faf5ba42128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfRKILmA2-3j"
      },
      "source": [
        "**Again, not much performance difference from model 5, but training time increased 2 times**. We have covered all the steps from deciding the architecture. **We select model 5 to be the apt architecture, which we will take forward**.<br><br>  Now we will tune for better hyper-parameters starting with **trying a different optimizer:**<br> \r\n",
        "Untill now we have been using Adams optimizer but lets see the performance with RMSProp. As we said, model architecture of model 5 is our choice and from now on we will work on model 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTn380_Ag7As",
        "outputId": "f3d5a7f4-5bc7-4113-ee21-42dd237a179b"
      },
      "source": [
        "# model5 architecture \r\n",
        "input = Input(shape=(x_train.shape[1],),batch_size=None)\r\n",
        "model5 = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train.shape[1], trainable=True)(input)\r\n",
        "model5 = LSTM(units=100, return_sequences=True,recurrent_dropout=0.2)(model5)\r\n",
        "model5 = GlobalMaxPool1D()(model5)\r\n",
        "model5 = Dropout(0.2)(model5)\r\n",
        "out = Dense(y_train.shape[1], activation=\"softmax\")(model5)\r\n",
        "model5 = Model(input, out)\r\n",
        "model5.summary()\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "# train model5\r\n",
        "train_model(model5, x_train, y_train, x_test, y_test, optm='rmsprop', ep=15, bs=16) #rmsprop optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 76)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 76, 200)           3847000   \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 76, 100)           120400    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_6 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 74)                7474      \n",
            "=================================================================\n",
            "Total params: 3,974,874\n",
            "Trainable params: 3,974,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "Epoch 1/15\n",
            "402/402 [==============================] - 62s 149ms/step - loss: 2.6985 - accuracy: 0.4580 - val_loss: 1.9960 - val_accuracy: 0.5529\n",
            "Epoch 2/15\n",
            "402/402 [==============================] - 60s 149ms/step - loss: 1.8657 - accuracy: 0.5786 - val_loss: 1.6991 - val_accuracy: 0.6063\n",
            "Epoch 3/15\n",
            "402/402 [==============================] - 60s 149ms/step - loss: 1.5930 - accuracy: 0.6274 - val_loss: 1.5967 - val_accuracy: 0.6194\n",
            "Epoch 4/15\n",
            "402/402 [==============================] - 61s 151ms/step - loss: 1.3767 - accuracy: 0.6742 - val_loss: 1.4806 - val_accuracy: 0.6325\n",
            "Epoch 5/15\n",
            "402/402 [==============================] - 60s 150ms/step - loss: 1.2168 - accuracy: 0.7032 - val_loss: 1.4238 - val_accuracy: 0.6505\n",
            "Epoch 6/15\n",
            "402/402 [==============================] - 60s 149ms/step - loss: 1.0522 - accuracy: 0.7321 - val_loss: 1.3897 - val_accuracy: 0.6586\n",
            "Epoch 7/15\n",
            "402/402 [==============================] - 60s 148ms/step - loss: 0.9509 - accuracy: 0.7686 - val_loss: 1.3643 - val_accuracy: 0.6654\n",
            "Epoch 8/15\n",
            "402/402 [==============================] - 60s 149ms/step - loss: 0.8513 - accuracy: 0.7900 - val_loss: 1.3488 - val_accuracy: 0.6741\n",
            "Epoch 9/15\n",
            "402/402 [==============================] - 60s 150ms/step - loss: 0.7109 - accuracy: 0.8188 - val_loss: 1.3550 - val_accuracy: 0.6810\n",
            "Epoch 10/15\n",
            "402/402 [==============================] - 60s 149ms/step - loss: 0.6484 - accuracy: 0.8376 - val_loss: 1.3353 - val_accuracy: 0.6841\n",
            "Epoch 11/15\n",
            "402/402 [==============================] - 60s 149ms/step - loss: 0.5854 - accuracy: 0.8562 - val_loss: 1.3467 - val_accuracy: 0.6785\n",
            "Epoch 12/15\n",
            "402/402 [==============================] - 60s 148ms/step - loss: 0.5370 - accuracy: 0.8751 - val_loss: 1.3812 - val_accuracy: 0.6797\n",
            "Epoch 13/15\n",
            "402/402 [==============================] - 60s 148ms/step - loss: 0.4526 - accuracy: 0.8944 - val_loss: 1.3680 - val_accuracy: 0.6810\n",
            "Epoch 14/15\n",
            "402/402 [==============================] - 59s 148ms/step - loss: 0.4366 - accuracy: 0.9000 - val_loss: 1.4013 - val_accuracy: 0.6785\n",
            "Epoch 15/15\n",
            "402/402 [==============================] - 60s 148ms/step - loss: 0.4014 - accuracy: 0.9063 - val_loss: 1.3979 - val_accuracy: 0.6797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7fa659f3f390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5TTilcD5Z5B"
      },
      "source": [
        "**Not much performance difference from adam, hence we will go ahead with adam only**\r\n",
        "### In the next part we will tune the learning rate and train for more epochs with callbacks to further avoid overfitting. "
      ]
    }
  ]
}